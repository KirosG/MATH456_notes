[
["index.html", "Applied Statistics II Preface", " Applied Statistics II Robin A. Donatello and Edward A. Roualdes 2018-01-09 Preface This document is a set of course notes for Math 456 - Applied Statistics II at California State University, Chico. This is not a textbook replacement, and topics covered will vary depending on the instructor. To make this clear we use the term notebook to refer to this document so as not to be confused with a traditional textbook. Some data and examples in this notebook are drawn from Practical Multivariate Analysis, 5th ed, Afifi, May, Clark and used with permission by the authors. This notebook was created using the bookdown package by Yihui Xie. "],
["prepare.html", "Chapter 1 Preparing Data for Analysis ", " Chapter 1 Preparing Data for Analysis "],
["reproducible-workflows.html", "1.1 Reproducible Workflows", " 1.1 Reproducible Workflows PrepareData You are your own collaborator 6 months from now. Make sure you will be able to understand what you were doing. Investing the time to do things clearly and in a reproducible manner will make your future self happy. Comment your code with explanations and instructions. How did you get from point A to B? Why did you recode this variable in this manner? This is reason #1 we use the Markdown language through R. Repro Figure Credits: Roger Peng "],
["identifying-variable-types.html", "1.2 Identifying Variable Types", " 1.2 Identifying Variable Types This section uses the raw depression data set from Afifi et.al. Consider a variable that measures marital status. What data type does R see this variable as? table(depress$MARITAL) ## ## 1 2 3 4 5 ## 73 127 43 13 38 str(depress$MARITAL) ## int [1:294] 5 3 2 3 4 2 2 1 2 2 ... class(depress$MARITAL) ## [1] &quot;integer&quot; When variables have numerical levels it is necessary to ensure that R knows it is a factor variable. The following code uses the factor() function to take the marital status variable and convert it into a factor variable with specified labels that match the codebook. depress$MARITAL &lt;- factor(depress$MARITAL, labels = c(&quot;Never Married&quot;, &quot;Married&quot;, &quot;Divorced&quot;, &quot;Separated&quot;, &quot;Widowed&quot;)) It is important to confirm the recode worked. If it did not you will have to re-read in the raw data set again since the variable SEX was replaced. table(depress$MARITAL) ## ## Never Married Married Divorced Separated Widowed ## 73 127 43 13 38 class(depress$MARITAL) ## [1] &quot;factor&quot; Create a boxplot of income across marital status category. qplot(y=INCOME, x=MARITAL, data=depress, geom=&quot;boxplot&quot;) Boxplots are nice because they clearly show the range where 50% of the data lie and any potential outliers. Boxplots can also indicate skewness, but sometimes it is helpful to visualize the location of the mean as well as the median. ggplot2 has a nice stat_summary layer that will calculate and add the means to the current plot. qplot(y=INCOME, x=MARITAL, data=depress, geom=&quot;boxplot&quot;) + stat_summary(fun.y=mean, colour=&quot;blue&quot;, size=3, geom=&quot;point&quot;) "],
["data-editing-and-recoding.html", "1.3 Data Editing and Recoding", " 1.3 Data Editing and Recoding For unbiased and accurate results of a statistical analysis, sufficient data has to be present. Often times once you start slicing and dicing the data to only look at certain groups, or if you are interested in the behavior of certain variables across levels of another variable, sometimes you start to run into small sample size problems. For example, consider marital status again: table(depress$MARITAL) ## ## Never Married Married Divorced Separated Widowed ## 73 127 43 13 38 There are only 13 people who report being separated. This could potentially be too small of a group size for valid statistical analysis. One way to deal with insufficient data within a certain category is to collapse categories. The following code uses the recode() function from the car package to create a new variable that I am calling MARITAL2 that combines the Divorced and Separated levels. library(car) ## ## Attaching package: &#39;car&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## recode depress$MARITAL2 &lt;- recode(depress$MARITAL, &quot;&#39;Divorced&#39; = &#39;Sep/Div&#39;; &#39;Separated&#39; = &#39;Sep/Div&#39;&quot;) Always confirm your recodes. table(depress$MARITAL, depress$MARITAL2, useNA=&quot;always&quot;) ## ## Married Never Married Sep/Div Widowed &lt;NA&gt; ## Never Married 0 73 0 0 0 ## Married 127 0 0 0 0 ## Divorced 0 0 43 0 0 ## Separated 0 0 13 0 0 ## Widowed 0 0 0 38 0 ## &lt;NA&gt; 0 0 0 0 0 This confirms that records where MARITAL (rows) is Divorced or Separated have the value of Sep/Div for MARITAL2 (columns). Now let’s examine the relationship between income against marital status by creating a boxplot. This is a situation where jittering or dodging the points is helpful to avoid overplotting of points. Note that the full ggplot code had to be used here, not the simpler qplot methods. Furthermore, the grid.arrange function from the gridExtra package is used to display these plots side by side. library(gridExtra) ## ## Attaching package: &#39;gridExtra&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## combine a &lt;- qplot(x=MARITAL2, y=INCOME, data=depress, col=MARITAL2, geom=&quot;point&quot;, main = &quot;Without jittering&quot;) + coord_flip() + theme(legend.position=&quot;none&quot;) b &lt;- ggplot(depress, aes(x=INCOME, y=MARITAL2, color=MARITAL2), main=&quot;With jittering&quot;) + geom_point(position=position_jitter()) + theme(legend.position=&quot;none&quot;) grid.arrange(a, b, ncol=2) What do you think coord_flip() does? Look at the difference in the X and Y values between plot a and plot b. What do you think theme(legend.position=&quot;none&quot;) does? Hint: Try removing them and see what happens. What can you say about the relationship between Income and marital status? "],
["outliers.html", "1.4 Outliers", " 1.4 Outliers Let’s look at the age variable in the depression data set. par(mfrow=c(1,2)) boxplot(depress$AGE) hist(depress$AGE) Just looking at the data graphically raises no red flags. The boxplot shows no outlying values and the histogram does not look wildly skewed. This is where knowledge about the data set is essential. The codebook does not provide a valid range for the data, but the description of the data starting on page 3 in the textbook clarifies that this data set is on adults. In the research world, this specifies 18 years or older. Now look back at the graphics. See anything odd? It appears as if the data go pretty far below 20, possibly below 18. Let’s check the numerical summary to get more details. summary(depress$AGE) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 9.00 28.00 42.50 44.38 59.00 89.00 The minimum value is a 9, which is outside the range of valid values for this variable. This is where you, as a statistician, data analyst or researcher goes back to the PI and asks for advice. Should this data be set to missing, or edited in a way that changes this data point into a valid piece of data. As an example of a common data entry error, and for demonstration purposes, I went in and changed a 19 to a 9. So the correct thing to do here is to change that 9, back to a 19. This is a very good use of the ifelse() function. depress$AGE &lt;- ifelse(depress$AGE==9, 19, depress$AGE) The logical statement is depress$AGE==9. Wherever this is true, replace the value of depress$AGE with 19, wherever this is false then keep the value of depress$AGE unchanged (by “replacing” the new value with the same old value). Alternatively, you can change that one value using bracket notation. Here you are specifying that you only want the rows where AGE==9, and directly assign a value of 19 to those rows. depress$AGE[depress$AGE==9] &lt;- 19 Confirm the recode. summary(depress$AGE) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 18.00 28.00 42.50 44.41 59.00 89.00 Looks like it worked. "],
["data-transformations.html", "1.5 Data Transformations", " 1.5 Data Transformations Let’s look at assessing normal distributions using the cleaned depression data set. rm(depress) # remove the current version that was used in the previous part of this markdown file depress &lt;- read.table(&quot;C:/GitHub/MATH456/data/Depress_020116.txt&quot;, sep=&quot;\\t&quot;, header=TRUE) hist(depress$INCOME, prob=TRUE, xlab=&quot;Annual income (in thousands)&quot;, main=&quot;Histogram and Density curve of Income&quot;, ylab=&quot;&quot;) lines(density(depress$INCOME), col=&quot;blue&quot;) summary(depress$INCOME) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 2.00 9.00 15.00 20.57 28.00 65.00 The distribution of annual income is slightly skewed right with a mean of $20.5k per year and a median of $15k per year income. The range of values goes from $2k to $65k. Reported income above $40k appear to have been rounded to the nearest $10k, because there are noticeable peaks at $40k, $50k, and $60k. In general, transformations are more effective when the the standard deviation is large relative to the mean. One rule of thumb is if the sd/mean ratio is less than 1/4, a transformation may not be necessary. sd(depress$INCOME) / mean(depress$INCOME) ## [1] 0.743147 Alternatively Hoaglin, Mosteller and Tukey (1985) showed that if the largest observation divided by the smallest observation is over 2, then the data may not be sufficiently variable for the transformation to be decisive. max(depress$INCOME) / (min(depress$INCOME)+.1) ## [1] 30.95238 Note these rules are not meaningful for data without a natural zero. Another common method of assessing normality is to create a normal probability (or normal quantile) plot. qqnorm(depress$INCOME);qqline(depress$INCOME, col=&quot;red&quot;) The points on the normal probability plot do not follow the red reference line very well. The dots show a more curved, or U shaped form rather than following a linear line. This is another indication that the data is skewed and a transformation for normality should be created. Create three new variables: log10inc as the log base 10 of Income, loginc as the natural log of Income, and xincome which is equal to the negative of one divided by the cubic root of income. log10inc &lt;- log10(depress$INCOME) loginc &lt;- log(depress$INCOME) xincome &lt;- -1/(depress$INCOME)^(-1/3) Create a single plot that display normal probability plots for the original, and each of the three transformations of income. Use the base graphics grid organizer par(mfrow=c(r,c)) where r is the number of rows and c is the number of columns. Which transformation does a better job of normalizing the distribution of Income? par(mfrow=c(2,2)) # Try (4,1) and (1,4) to see how this works. qqnorm(depress$INCOME, main=&quot;Income&quot;); qqline(depress$INCOME,col=&quot;blue&quot;) qqnorm(log10inc, main=&quot;Log 10&quot;); qqline(log10inc, col=&quot;blue&quot;) qqnorm(loginc, main = &quot;Natural Log&quot;); qqline(loginc, col=&quot;blue&quot;) qqnorm(xincome, main=&quot;-1/cuberoot(income)&quot;); qqline(xincome, col=&quot;blue&quot;) "],
["selecting-appropriate-analysis.html", "1.6 Selecting Appropriate Analysis", " 1.6 Selecting Appropriate Analysis Considerations: Purpose of analysis. Types of variables in data set. Data used in analysis. Assumptions needed; satisfied? Choice of analyses is often arbitrary: consider several Example: 5 independent variables: 3 interval, 1 ordinal, 1 nominal 1 dependent variable: interval Analysis options Multiple regression: pretend independent ordinal variable is an interval variable use dummy (0 /1) variables for nominal variables Analysis of variance: categorize all independent variables Analysis of covariance: leave variables as is, check assumptions Logistic regression: Categorize dependent variable: high, low Survival analysis: IF dependent variable is time to an event Unsure? Do several and compare results. "],
["wide-vs-long-data.html", "1.7 Wide vs. Long data", " 1.7 Wide vs. Long data The data on Lung function originally was recorded in wide format, with separate variables for mother’s and father’s FEV1 score (MFEV1 and FFEV). In this format, the data is one record per family. fev &lt;- read.delim(&quot;https://norcalbiostat.netlify.com/data/Lung_081217.txt&quot;, sep=&quot;\\t&quot;, header=TRUE) head(fev) ## ID AREA FSEX FAGE FHEIGHT FWEIGHT FFVC FFEV1 MSEX MAGE MHEIGHT MWEIGHT ## 1 1 1 1 53 61 161 391 3.23 2 43 62 136 ## 2 2 1 1 40 72 198 441 3.95 2 38 66 160 ## 3 3 1 1 26 69 210 445 3.47 2 27 59 114 ## 4 4 1 1 34 68 187 433 3.74 2 36 58 123 ## 5 5 1 1 46 61 121 354 2.90 2 39 62 128 ## 6 6 1 1 44 72 153 610 4.91 2 36 66 125 ## MFVC MFEV1 OCSEX OCAGE OCHEIGHT OCWEIGHT OCFVC OCFEV1 MCSEX MCAGE ## 1 370 3.31 2 12 59 115 296 2.79 NA NA ## 2 411 3.47 1 10 56 66 323 2.39 NA NA ## 3 309 2.65 1 8 50 59 114 1.11 NA NA ## 4 265 2.06 2 11 57 106 256 1.85 1 9 ## 5 245 2.33 1 16 61 88 260 2.47 2 12 ## 6 349 3.06 1 15 67 100 389 3.55 1 13 ## MCHEIGHT MCWEIGHT MCFVC MCFEV1 YCSEX YCAGE YCHEIGHT YCWEIGHT YCFVC ## 1 NA NA NA NA NA NA NA NA NA ## 2 NA NA NA NA NA NA NA NA NA ## 3 NA NA NA NA NA NA NA NA NA ## 4 49 56 159 1.30 NA NA NA NA NA ## 5 60 85 268 2.34 2 10 50 53 154 ## 6 57 87 276 2.37 2 10 55 72 195 ## YCFEV1 ## 1 NA ## 2 NA ## 3 NA ## 4 NA ## 5 1.43 ## 6 1.69 To analyze the effect of gender on FEV, the data need to be in long format, with a single variable for FEV and a separate variable for gender. The following code chunk demonstrates one method of combining data on height, gender, age and FEV1 for both males and females. fev2 &lt;- data.frame(gender = c(fev$FSEX, fev$MSEX), FEV = c(fev$FFEV1, fev$MFEV1), ht = c(fev$FHEIGHT, fev$MHEIGHT), age = c(fev$FAGE, fev$MAGE)) fev2$gender &lt;- factor(fev2$gender, labels=c(&quot;M&quot;, &quot;F&quot;)) head(fev2) ## gender FEV ht age ## 1 M 3.23 61 53 ## 2 M 3.95 72 40 ## 3 M 3.47 69 26 ## 4 M 3.74 68 34 ## 5 M 2.90 61 46 ## 6 M 4.91 72 44 Nearly all analysis procedures and most graphing procedures require the data to be in long format. There are several R packages that can help with this including reshape2 and tidyr. "],
["linreg.html", "Chapter 2 Linear Regression", " Chapter 2 Linear Regression The general purpose of regression is to learn more about the relationship between several independent or predictor variables and a quantitative dependent variable. Multiple regression procedures are very widely used in research. In general, this inferential tool allows us to ask (and hopefully answer) the general question “what is the best predictor of…”, and does “additional variable A” or “additional variable B” confound the relationship between my explanatory and response variable?” Educational researchers might want to learn about the best predictors of success in high-school. Sociologists may want to find out which of the multiple social indicators best predict whether or not a new immigrant group will adapt to their new country of residence. Biologists may want to find out which factors (i.e. temperature, barometric pressure, humidity, etc.) best predict caterpillar reproduction. This chapter starts by recapping notation and topics for simple linear regression, when there is only one predictor. Then we move into generalization of these concepts to many predictors, and model building topics such as stratification, interactions, and categorical predictors. "],
["simple-linear-regression.html", "2.1 Simple Linear Regression", " 2.1 Simple Linear Regression The goal of linear regression is to Describe the relationship between an independent variable X and a continuous dependent variable \\(Y\\) as a straight line. The textbook discusses two cases: Fixed-\\(X\\): values of \\(X\\) are preselected by investigator Variable-\\(X\\): have random sample of \\((X,Y)\\) values Calculations are the same, Draw inferences regarding this relationship Predict value of \\(Y\\) for a given value of \\(X\\) 2.1.1 Mathmatical Model The mean of \\(Y\\) values at any given \\(X\\) is \\(\\beta_{0} + \\beta_{1} X\\) The variance of \\(Y\\) values at any \\(X\\) is \\(\\sigma^2\\) (same for all X) \\(Y\\) values are normally distributed at any given \\(X\\) (need for inference) Figure 6.2 2.1.2 Parameter Estimates Estimate the slope \\(\\beta_{1}\\) and intercept \\(\\beta_{0}\\) using least-squares methods. The residual mean squared error (RMSE) is an estimate of the variance \\(s^{2}\\) Typically interested in inference on \\(\\beta_{1}\\) Assume no relationship between \\(X\\) and \\(Y\\) \\((H_{0}: \\beta_{1}=0)\\) until there is reason to believe there is one \\((H_{0}: \\beta_{1} \\neq 0)\\) 2.1.3 Interval estimation Everything is estimated with some degree of error Confidence intervals for the mean of \\(Y\\) Prediction intervals for an individual \\(Y\\) Which one is wider? Why? 2.1.4 Corelation Coefficient The correlation coefficient \\(\\rho\\) measures the strength of association between \\(X\\) and \\(Y\\) in the population. \\(\\sigma^{2} = VAR(Y|X)\\) is the variance of \\(Y\\) for a specific \\(X\\). \\(\\sigma_{y}^{2} = VAR(Y)\\) is the variance of \\(Y\\) for all \\(X\\)’s. \\[ \\sigma^{2} = \\sigma_{y}^{2}(1-\\rho^{2})\\] \\[ \\rho^{2} = \\frac{\\sigma_{y}^{2} - \\sigma^{2}}{\\sigma_{y}^{2}}\\] \\(\\rho^{2}\\) = reduction in variance of Y associated with knowledge of X/original variance of Y Coefficient of Determiniation: \\(100\\rho^{2}\\) = % of variance of Y associated with X or explained by X Caution: association vs. causation. 2.1.5 Assumptions Homogeneity of variance (same \\(\\sigma^{2}\\)) Not extremely serious Can use transformations to achieve it Graphical assessment: Plot the residuals against the x variable, add a lowess line. This assumption is upheld if there is no relationship/trend between the residuals and the predictor. Normal residuals Slight departures OK Can use transformations to achieve it Graphical assessment: normal qqplot of the model residuals. Randomness / Independence Very serious Can use hierarchical models for clustered samples No real good way to “test” for independence. Need to know how the sample was obtained. Linear relationship Slight departures OK Can use transformations to achieve it Graphical assessment: Simple scatterplot of \\(y\\) vs \\(x\\). Looking for linearity in the relationship. Should be done prior to any analysis. 2.1.6 Example {slr-ex} Using a cleaned version of the Lung function data set, lets explore the relationship between height and FEV for fathers in this data set. fev &lt;- read.delim(&quot;https://norcalbiostat.netlify.com/data/Lung_081217.txt&quot;, sep=&quot;\\t&quot;, header=TRUE) library(ggplot2) qplot(y=FFEV1, x=FHEIGHT, geom=&quot;point&quot;, data=fev, xlab=&quot;Height&quot;, ylab=&quot;FEV1&quot;, main=&quot;Scatter Diagram with Regression (blue) and Lowess (red) Lines of FEV1 Versus Height for Fathers.&quot;) + geom_smooth(method=&quot;lm&quot;, se=FALSE, col=&quot;blue&quot;) + geom_smooth(se=FALSE, col=&quot;red&quot;) ## `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; There does appear to be a tendency for taller men to have higher FEV1. Let’s fit a linear model and report the regression parameter estimates. model &lt;- lm(FFEV1 ~ FHEIGHT, data=fev) summary(model) ## ## Call: ## lm(formula = FFEV1 ~ FHEIGHT, data = fev) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.56688 -0.35290 0.04365 0.34149 1.42555 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -4.08670 1.15198 -3.548 0.000521 *** ## FHEIGHT 0.11811 0.01662 7.106 4.68e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5638 on 148 degrees of freedom ## Multiple R-squared: 0.2544, Adjusted R-squared: 0.2494 ## F-statistic: 50.5 on 1 and 148 DF, p-value: 4.677e-11 The least squares equation is \\(Y = -4.087 + 0.118X\\). confint(model) ## 2.5 % 97.5 % ## (Intercept) -6.36315502 -1.8102499 ## FHEIGHT 0.08526328 0.1509472 For ever inch taller a father is, his FEV1 measurement significantly increases by .12 (95%CI: .09, .15, p&lt;.0001). The correlation between FEV1 and height is \\(\\sqrt{.2544}\\) = 0.5. Lastly, check assumptions on the residuals to see if the model results are valid. Homogeneity of variance plot(model$residuals ~ fev$FHEIGHT) lines(lowess(model$residuals ~ fev$FHEIGHT), col=&quot;red&quot;) Normal residuals qqnorm(model$residuals) qqline(model$residuals, col=&quot;red&quot;) No major deviations away from what is expected. "],
["multiple-linear-regression.html", "2.2 Multiple Linear Regression", " 2.2 Multiple Linear Regression Extends simple linear regression. Describes a linear relationship between a single continuous \\(Y\\) variable, and several \\(X\\) variables. Predicts \\(Y\\) from \\(X_{1}, X_{2}, \\ldots , X_{P}\\). Now it’s no longer a 2D regression line, but a \\(p\\) dimensional regression plane. 2.2.1 Types of X variables Fixed: The levels of \\(X\\) are selected in advance with the intent to measure the affect on an outcome \\(Y\\). Variable: Random sample of individuals from the population is taken and \\(X\\) and \\(Y\\) are measured on each individual. X’s can be continuous or discrete (categorical) X’s can be transformations of other X’s, e.g., \\(log(x), x^{2}\\). 2.2.2 Mathematical Model \\[ y_{i} = \\beta_{0} + \\beta_{1}x_{1i} + \\ldots + \\beta_{p}x_{pi} + \\epsilon_{i}\\] The assumptions on the residuals \\(\\epsilon_{i}\\) still hold: They have mean zero They are homoscedastic, that is all have the same finite variance: \\(Var(\\epsilon_{i})=\\sigma^{2}&lt;\\infty\\) Distinct error terms are uncorrelated: (Independent) \\(\\text{Cov}(\\epsilon_{i},\\epsilon_{j})=0,\\forall i\\neq j.\\) The regression model relates \\(y\\) to a function of \\(\\textbf{X}\\) and \\(\\mathbf{\\beta}\\), where \\(\\textbf{X}\\) is a \\(nxp\\) matrix of \\(p\\) covariates on \\(n\\) observations and \\(\\mathbf{\\beta}\\) is a length \\(p\\) vector of regression coefficients. In matrix notation this looks like: \\[ \\textbf{y} = \\textbf{X} \\mathbf{\\beta} + \\mathbf{\\epsilon} \\] 2.2.3 Parameter Estimation The goal of regression analysis is to minimize the residual error. That is, to minimize the difference between the value of the dependent variable predicted by the model and the true value of the dependent variable. \\[ \\epsilon_{i} = \\hat{y_{i}} - y_{i}\\] The method of Least Squares accomplishes this by finding parameter estimates \\(\\beta_{0}\\) and \\(\\beta_{1}\\) that minimized the sum of the squared residuals: \\[ \\sum_{i=1}^{n} \\epsilon_{i} \\] For simple linear regression the regression coefficient estimates that minimize the sum of squared errors can be calculated as: \\[ \\hat{\\beta_{0}} = \\bar{y} - \\hat{\\beta_{1}}\\bar{x} \\quad \\mbox{ and } \\quad \\hat{\\beta_{1}} = r\\frac{s_{y}}{s_{x}} \\] For multiple linear regression, the fitted values \\(\\hat{y_{i}}\\) are calculated as the linear combination of x’s and \\(\\beta\\)’s, \\(\\sum_{i=1}^{p}X_{ij}\\beta_{j}\\). The sum of the squared residual errors (the distance between the observed point \\(y_{i}\\) and the fitted value) now has the following form: \\[ \\sum_{i=1}^{n} |y_{i} - \\sum_{i=1}^{p}X_{ij}\\beta_{j}|^{2}\\] Or in matrix notation \\[ || \\mathbf{y} - \\mathbf{X}\\mathbf{\\beta} ||^{2} \\] The details of methods to calculate the Least Squares estimate of \\(\\beta\\)’s is left to a course in mathematical statistics. 2.2.4 Example {mlr-ex} The analysis in example (???) concluded that FEV1 in fathers significantly increases by 0.12 (95% CI:0.09, 0.15) liters per additional inch in height (p&lt;.0001). Looking at the multiple \\(R^{2}\\) (correlation of determination), this simple model explains 25% of the variance seen in the outcome \\(y\\). However, FEV tends to decrease with age for adults, so we should be able to predict it better if we use both height and age as independent variables in a multiple regression equation. What direction do you expect the slope coefficient for age to be? For height? Fitting a regression model in R with more than 1 predictor is done by adding each variable to the right hand side of the model notation connected with a +. mv_model &lt;- lm(FFEV1 ~ FAGE + FHEIGHT, data=fev) summary(mv_model) ## ## Call: ## lm(formula = FFEV1 ~ FAGE + FHEIGHT, data = fev) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.34708 -0.34142 0.00917 0.37174 1.41853 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.760747 1.137746 -2.427 0.0165 * ## FAGE -0.026639 0.006369 -4.183 4.93e-05 *** ## FHEIGHT 0.114397 0.015789 7.245 2.25e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5348 on 147 degrees of freedom ## Multiple R-squared: 0.3337, Adjusted R-squared: 0.3247 ## F-statistic: 36.81 on 2 and 147 DF, p-value: 1.094e-13 confint(mv_model) ## 2.5 % 97.5 % ## (Intercept) -5.00919751 -0.51229620 ## FAGE -0.03922545 -0.01405323 ## FHEIGHT 0.08319434 0.14559974 Holding height constant, a father who is one year older is expected to have a FEV value 0.03 (0.01, 0.04) liters less than another man (p&lt;.0001). Holding height constant, a father who is 1cm taller than another man is expected to have a FEV value of 0.11 (.08, 0.15) liter greater than the other man (p&lt;.0001). For the model that includes age, the coefficient for height is now 0.11, which is interpreted as the rate of change of FEV1 as a function of height after adjusting for age. This is also called the partial regression coefficient of FEV1 on height after adjusting for age. Both height and age are significantly associated with FEV in fathers (p&lt;.0001 each). "],
["model-diagnostics.html", "2.3 Model Diagnostics", " 2.3 Model Diagnostics The same set of regression diagnostics can be examined to identify any potential influential points, outliers or other problems with the linear model. par(mfrow=c(2,2)) plot(mv_model) "],
["multicollinearity.html", "2.4 Multicollinearity", " 2.4 Multicollinearity Occurs when some of the X variables are highly intercorrelated. Affects estimates and their SE’s (p. 143) Look at tolerance, and its inverse, the Variance Inflation Factor (VIF) Need tolerance &lt; 0.01, or VIF &gt; 100. library(car) vif(mv_model) ## FAGE FHEIGHT ## 1.003163 1.003163 tolerance = 1/vif(mv_model) tolerance ## FAGE FHEIGHT ## 0.9968473 0.9968473 Solution: use variable selection to delete some X variables. Alternatively, use dimension reduction techniques such as Principal Components "],
["what-to-watch-out-for.html", "2.5 What to watch out for", " 2.5 What to watch out for Representative sample Range of prediction should match observed range of X in sample Use of nominal or ordinal, rather than interval or ratio data Errors-in-variables Correlation does not imply causation Violation of assumptions Influential points Appropriate model Multicollinearity "],
["model-building.html", "Chapter 3 Model Building", " Chapter 3 Model Building Model building methods are used mainly in exploratory situations where many independent variables have been measured, but a final model explaining the dependent variable has not been reached. You want to build a model that contains enough covariates to explain the model well, but still be parsimonious such that the model is still interpretable. This chapter introduces categorical predictors first, then interactions, moderating and confounding variables, and then discusses model fit measures that can be used to compare between competing models. "],
["categorical-predictors.html", "3.1 Categorical Predictors", " 3.1 Categorical Predictors Let’s continue to model the length of the iris petal based on the length of the sepal, controlling for species. But here we’ll keep species as a categorical variable. What happens if we just put the variable in the model? summary(lm(Petal.Length ~ Sepal.Length + Species, data=iris)) ## ## Call: ## lm(formula = Petal.Length ~ Sepal.Length + Species, data = iris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.76390 -0.17875 0.00716 0.17461 0.79954 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.70234 0.23013 -7.397 1.01e-11 *** ## Sepal.Length 0.63211 0.04527 13.962 &lt; 2e-16 *** ## Speciesversicolor 2.21014 0.07047 31.362 &lt; 2e-16 *** ## Speciesvirginica 3.09000 0.09123 33.870 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2826 on 146 degrees of freedom ## Multiple R-squared: 0.9749, Adjusted R-squared: 0.9744 ## F-statistic: 1890 on 3 and 146 DF, p-value: &lt; 2.2e-16 Examine the coefficient names, Speciesversicolor and Speciesvirginica. R (and most software packages) automatically take a categorical variable and turn it into a series of binary indicator variables. Let’s look at what the software program does in the background. Below is a sample of the iris data. The first column shows the row number, specifically I am only showing 2 sample rows from each species. The second column is the value of the sepal length, the third is the binary indicator for if the iris is from species versicolor, next the binary indicator for if the iris is from species virginica, and lastly the species as a 3 level categorical variable (which is what we’re used to seeing at this point.) ## Warning: package &#39;pander&#39; was built under R version 3.3.3 Sepal.Length Speciesversicolor Speciesvirginica Species 1 5.1 0 0 setosa 2 4.9 0 0 setosa 51 7 1 0 versicolor 52 6.4 1 0 versicolor 101 6.3 0 1 virginica 102 5.8 0 1 virginica "],
["factor-variable-coding.html", "3.2 Factor variable coding", " 3.2 Factor variable coding Most commonly known as “Dummy coding”. Not an informative term to use. Better used term: Indicator variable Math notation: I(gender == “Female”). A.k.a reference coding For a nominal X with K categories, define K indicator variables. Choose a reference (referent) category: Leave it out Use remaining K-1 in the regression. Often, the largest category is chosen as the reference category. For the iris example, 2 indicator variables are created for versicolor and virginica. Interpreting the regression coefficients are going to be compared to the reference group. In this case, it is species setosa. The mathematical model is now written as follows, where \\(x_{1}\\) is Sepal Length, \\(x_{2}\\) is the indicator for versicolor, and \\(x_{3}\\) the indicator for virginica \\[ Y_{i} \\sim \\beta_{0} + \\beta_{1}x_{i} + \\beta_{2}x_{2i} + \\beta_{3}x_{3i}+ \\epsilon_{i}\\] Let’s look at the regression coefficients and their 95% confidence intervals from the main effects model again. main.eff.model &lt;- lm(Petal.Length ~ Sepal.Length + Species, data=iris) pander(main.eff.model) Fitting linear model: Petal.Length ~ Sepal.Length + Species Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -1.702 0.2301 -7.397 1.005e-11 Sepal.Length 0.6321 0.04527 13.96 1.121e-28 Speciesversicolor 2.21 0.07047 31.36 9.646e-67 Speciesvirginica 3.09 0.09123 33.87 4.918e-71 pander(confint(main.eff.model)) 2.5 % 97.5 % (Intercept) -2.157 -1.248 Sepal.Length 0.5426 0.7216 Speciesversicolor 2.071 2.349 Speciesvirginica 2.91 3.27 In this main effects model, Species only changes the intercept. The effect of species is not multiplied by Sepal length. The interpretations are the following: \\(b_{1}\\): After controlling for species, Petal length significantly increases with the length of the sepal (0.63, 95% CI 0.54-0.72, p&lt;.0001). \\(b_{2}\\): Versicolor has on average 2.2cm longer petal lengths compared to setosa (95% CI 2.1-2.3, p&lt;.0001). \\(b_{3}\\): Virginica has on average 3.1cm longer petal lengths compared to setosa (95% CI 2.9-3.3, p&lt;.0001). 3.2.1 Wald test The Wald test is used for simultaneous tests of \\(Q\\) variables in a model Consider a model with \\(P\\) variables and you want to test if \\(Q\\) additional variables are useful. \\(H_{0}: Q\\) additional variables are useless, i.e., their \\(\\beta\\)’s all = 0 \\(H_{A}: Q\\) additional variables are useful This can be done in R by using the regTermTest() function in the survey package. library(survey) ## Warning: package &#39;survey&#39; was built under R version 3.3.3 ## Loading required package: grid ## Loading required package: methods ## Loading required package: Matrix ## Warning: package &#39;Matrix&#39; was built under R version 3.3.3 ## Loading required package: survival ## ## Attaching package: &#39;survey&#39; ## The following object is masked from &#39;package:graphics&#39;: ## ## dotchart ## regTermTest(model.name, &quot;variable name to test&quot;) # not run 3.2.1.0.1 Example 1: Employment status on depression score Consider a model to predict depression using age, employment status and whether or not the person was chronically ill in the past year as covariates. This example uses the cleaned depression data set. depress &lt;- read.delim(&quot;https://norcalbiostat.netlify.com/data/depress_081217.txt&quot;, header=TRUE,sep=&quot;\\t&quot;) full_model &lt;- lm(cesd ~ age + chronill + employ, data=depress) pander(summary(full_model)) Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 11.48 1.502 7.646 3.191e-13 age -0.133 0.03514 -3.785 0.0001873 chronill 2.688 1.024 2.625 0.009121 employHouseperson 6.75 1.797 3.757 0.0002083 employIn School 1.967 5.995 0.328 0.7431 employOther 4.897 4.278 1.145 0.2533 employPT 3.259 1.472 2.214 0.02765 employRetired 3.233 1.886 1.714 0.08756 employUnemp 7.632 2.339 3.263 0.001238 Fitting linear model: cesd ~ age + chronill + employ Observations Residual Std. Error \\(R^2\\) Adjusted \\(R^2\\) 294 8.385 0.1217 0.09704 The results of this model show that age and chronic illness are statistically associated with CESD (each p&lt;.006). However employment status shows mixed results. Some employment statuses are significantly different from the reference group, some are not. So overall, is employment status associated with depression? Recall that employment is a categorical variable, and all the coefficient estimates shown are the effect of being in that income category has on depression compared to being employed full time. For example, the coefficient for PT employment is greater than zero, so they have a higher CESD score compared to someone who is fully employed. But what about employment status overall? Not all employment categories are significantly different from FT status. To test that employment status affects CESD we need to do a global test that all \\(\\beta\\)’s are 0. \\(H_{0}: \\beta_{3} = \\beta_{4} = \\beta_{5} = \\beta_{6} = \\beta_{7} = \\beta_{8} = 0\\) \\(H_{A}\\): At least one \\(\\beta_{j}\\) is not 0. regTermTest(full_model, &quot;employ&quot;) ## Wald test for employ ## in lm(formula = cesd ~ age + chronill + employ, data = depress) ## F = 4.153971 on 6 and 285 df: p= 0.0005092 Confirm that the degrees of freedom are correct. It should equal the # of categories in the variable you are testing, minus 1. Employment has 7 levels, so \\(df=6\\). Or equivelantly, the degrees of freedom are the number of \\(beta\\)’s you are testing to be 0. The p-value of this Wald test is significant, thus employment significantly predicts CESD score. 3.2.1.1 Example 2: Blood Pressure Consider a logistic model on smoking status (0= never smoked, 1=has smoked) using gender, income, and blood pressure class (bp_class) as predictors. \\[ logit(Y) = \\beta_{0} + \\beta_{1}\\mbox{(female)} + \\beta_{2}\\mbox{(income)} + \\beta_{3}\\mbox{(Pre-HTN)} + \\beta_{4}\\mbox{(HTN-I)} + \\beta_{5}\\mbox{(HTN-II)} \\] bp.mod &lt;- glm(smoke ~ female_c + income + bp_class, data=addhealth, family=&#39;binomial&#39;) pander(summary(bp.mod)) Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.046 0.1064 9.836 7.881e-23 female_cFemale -0.6182 0.07617 -8.117 4.798e-16 income -3.929e-06 1.411e-06 -2.785 0.005346 bp_classPre-HTN 0.07289 0.08206 0.8882 0.3745 bp_classHTN-I -0.02072 0.1093 -0.1895 0.8497 bp_classHTN-II 0.02736 0.1888 0.1449 0.8848 (Dispersion parameter for binomial family taken to be 1 ) Null deviance: 4853 on 3728 degrees of freedom Residual deviance: 4769 on 3723 degrees of freedom It is unlikely that blood pressure is associated with smoking status, all groups are not statistically significantly different from the reference group (all p-values are large). Let’s test that hypothesis formally using a Wald Test. regTermTest(bp.mod, &quot;bp_class&quot;) ## Wald test for bp_class ## in glm(formula = smoke ~ female_c + income + bp_class, family = &quot;binomial&quot;, ## data = addhealth) ## F = 0.428004 on 3 and 3723 df: p= 0.73294 The Wald Test has a large p-value of 0.73, thus blood pressure classification is not associated with smoking status. This means blood pressure classification should not be included in a model to explain smoking status. "],
["moderation.html", "3.3 Moderation", " 3.3 Moderation Moderation occurs when the relationship between two variables depends on a third variable. The third variable is referred to as the moderating variable or simply the moderator. The moderator affects the direction and/or strength of the relationship between the explanatory (\\(x\\)) and response (\\(y\\)) variable. This tends to be an important When testing a potential moderator, we are asking the question whether there is an association between two constructs, but separately for different subgroups within the sample. This is also called a stratified model, or a subgroup analysis. Here are 3 scenarios demonstrating how a third variable can modify the relationship between the original two variables. Scenario 1 - Significant relationship at bivariate level (saying expect the effect to exist in the entire population) then when test for moderation the third variable is a moderator if the strength (i.e., p-value is Non-Significant) of the relationship changes. Could just change strength for one level of third variable, not necessarily all levels of the third variable. Scenario 2 - Non-significant relationship at bivariate level (saying do not expect the effect to exist in the entire population) then when test for moderation the third variable is a moderator if the relationship becomes significant (saying expect to see it in at least one of the sub-groups or levels of third variable, but not in entire population because was not significant before tested for moderation). Could just become significant in one level of the third variable, not necessarily all levels of the third variable. Scenario 3 - Significant relationship at bivariate level (saying expect the effect to exist in the entire population) then when test for moderation the third variable is a moderator if the direction (i.e., means change order/direction) of the relationship changes. Could just change direction for one level of third variable, not necessarily all levels of the third variable. "],
["stratification.html", "3.4 Stratification", " 3.4 Stratification Stratified models examine the regression equations for each subgroup of the population and seeing if the relationship between the response and explanatory variables changed for at least one subgroup. Consider the relationship between the length of an iris petal, and the length of it’s sepal. Earlier we found that the iris species modified this relationship. Lets consider a binary indicator variable for species that groups veriscolor and virginica together. iris$setosa &lt;- ifelse(iris$Species==&quot;setosa&quot;, 1, 0) table(iris$setosa, iris$Species) ## ## setosa versicolor virginica ## 0 0 50 50 ## 1 50 0 0 Within the setosa species, there is little to no relationship between sepal and petal length. For the other two species, the relationship looks still significantly positive, but in the combined sample there appears to be a strong positive relationship (blue). library(ggplot2) ggplot(iris, aes(x=Sepal.Length, y=Petal.Length, col=as.factor(setosa))) + geom_point() + theme_bw() + theme(legend.position=&quot;top&quot;) + scale_color_manual(name=&quot;Species setosa&quot;, values=c(&quot;red&quot;, &quot;darkgreen&quot;)) + geom_smooth(se=FALSE, method=&quot;lm&quot;) + geom_smooth(aes(x=Sepal.Length, y=Petal.Length), col=&quot;blue&quot;, se=FALSE, method=&#39;lm&#39;) The mathematical model describing the relationship between Petal length (\\(Y\\)), and Sepal length (\\(X\\)), for species setosa (\\(s\\)) versus not-setosa (\\(n\\)), is written as follows: \\[ Y_{is} \\sim \\beta_{0s} + \\beta_{1s}*x_{i} + \\epsilon_{is} \\qquad \\epsilon_{is} \\sim \\mathcal{N}(0,\\sigma^{2}_{s})\\] \\[ Y_{in} \\sim \\beta_{0n} + \\beta_{1n}*x_{i} + \\epsilon_{in} \\qquad \\epsilon_{in} \\sim \\mathcal{N}(0,\\sigma^{2}_{n}) \\] In each model, the intercept, slope, and variance of the residuals can all be different. This is the unique and powerful feature of stratified models. The downside is that each model is only fit on the amount of data in that particular subset. Furthermore, each model has 3 parameters that need to be estimated: \\(\\beta_{0}, \\beta_{1}\\), and \\(\\sigma^{2}\\), for a total of 6 for the two models. The more parameters that need to be estimated, the more data we need. "],
["interactions.html", "3.5 Interactions", " 3.5 Interactions If we care about how species changes the relationship between petal and sepal length, we can fit a model with an interaction between sepal length (\\(x_{1}\\)) and species. For this first example let \\(x_{2}\\) be an indicator for when species == setosa . Note that both main effects of sepal length, and setosa species are also included in the model. Interactions are mathematically represented as a multiplication between the two variables that are interacting. \\[ Y_{i} \\sim \\beta_{0} + \\beta_{1}x_{i} + \\beta_{2}x_{2i} + \\beta_{3}x_{1i}x_{2i}\\] Ifwe evaluate this model for both levels of \\(x_{2}\\), the resulting models are the same as the stratified models. When \\(x_{2} = 0\\), the record is on an iris not from the setosa species. \\[ Y_{i} \\sim \\beta_{0} + \\beta_{1}x_{i} + \\beta_{2}(0) + \\beta_{3}x_{1i}(0)\\] which simplifies to \\[ Y_{i} \\sim \\beta_{0} + \\beta_{1}x_{i}\\] When \\(x_{2} = 1\\), the record is on an iris of the setosa species. \\[ Y_{i} \\sim \\beta_{0} + \\beta_{1}x_{i} + \\beta_{2}(1) + \\beta_{3}x_{1i}(1)\\] which simplifies to \\[ Y_{i} \\sim (\\beta_{0} + \\beta_{2}) + (\\beta_{1} + \\beta_{3})x_{i}\\] Each subgroup model has a different intercept and slope, but we had to estimate 4 parameters in the interaction model, and 6 for the fully stratified model. Interactions are fit in R by simply multiplying * the two variables together in the model statement. summary(lm(Petal.Length ~ Sepal.Length + setosa + Sepal.Length*setosa, data=iris)) ## ## Call: ## lm(formula = Petal.Length ~ Sepal.Length + setosa + Sepal.Length * ## setosa, data = iris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.96754 -0.19948 -0.01386 0.22597 1.05479 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.55571 0.37509 -4.148 5.68e-05 *** ## Sepal.Length 1.03189 0.05957 17.322 &lt; 2e-16 *** ## setosa 2.35877 0.88266 2.672 0.00839 ** ## Sepal.Length:setosa -0.90026 0.17000 -5.296 4.28e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3929 on 146 degrees of freedom ## Multiple R-squared: 0.9515, Adjusted R-squared: 0.9505 ## F-statistic: 954.1 on 3 and 146 DF, p-value: &lt; 2.2e-16 The coefficient \\(b_{3}\\) for the interaction term is significant, confirming that species changes the relationship between sepal length and petal length. 3.5.1 Interpretations summary(lm(Petal.Length ~ Sepal.Length + setosa + Sepal.Length*setosa, data=iris)) ## ## Call: ## lm(formula = Petal.Length ~ Sepal.Length + setosa + Sepal.Length * ## setosa, data = iris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.96754 -0.19948 -0.01386 0.22597 1.05479 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.55571 0.37509 -4.148 5.68e-05 *** ## Sepal.Length 1.03189 0.05957 17.322 &lt; 2e-16 *** ## setosa 2.35877 0.88266 2.672 0.00839 ** ## Sepal.Length:setosa -0.90026 0.17000 -5.296 4.28e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3929 on 146 degrees of freedom ## Multiple R-squared: 0.9515, Adjusted R-squared: 0.9505 ## F-statistic: 954.1 on 3 and 146 DF, p-value: &lt; 2.2e-16 The main effects (\\(b_{1}\\), \\(b_{2}\\)) cannot be interpreted by themselves when there is an interaction in the model. If \\(x_{2}=0\\), then the effect of \\(x_{1}\\) on \\(Y\\) simplifies to: \\(\\beta_{1}\\) \\(b_{1}\\) The effect of sepal length on petal length for non-setosa species of iris (setosa=0) For non-setosa species, the petal length increases 1.03cm for every additional cm of sepal length. If \\(x_{2}=1\\), then the effect of \\(x_{1}\\) on \\(Y\\) model simplifies to: \\(\\beta_{1} + \\beta_{3}\\) For setosa species, the petal length increases by 1.03-0.9=0.13 cm for every additional cm of sepal length. Let’s up the game now and look at the full interaction model with a categorical version of species. Recall \\(x_{1}\\) is Sepal Length, \\(x_{2}\\) is the indicator for versicolor, and \\(x_{3}\\) the indicator for virginica . \\[ Y_{i} \\sim \\beta_{0} + \\beta_{1}x_{i} + \\beta_{2}x_{2i} + \\beta_{3}x_{3i} + \\beta_{4}x_{1i}x_{2i} + \\beta_{5}x_{1i}x_{3i}+\\epsilon_{i}\\] summary(lm(Petal.Length ~ Sepal.Length + Species + Sepal.Length*Species, data=iris)) ## ## Call: ## lm(formula = Petal.Length ~ Sepal.Length + Species + Sepal.Length * ## Species, data = iris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.68611 -0.13442 -0.00856 0.15966 0.79607 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.8031 0.5310 1.512 0.133 ## Sepal.Length 0.1316 0.1058 1.244 0.216 ## Speciesversicolor -0.6179 0.6837 -0.904 0.368 ## Speciesvirginica -0.1926 0.6578 -0.293 0.770 ## Sepal.Length:Speciesversicolor 0.5548 0.1281 4.330 2.78e-05 *** ## Sepal.Length:Speciesvirginica 0.6184 0.1210 5.111 1.00e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2611 on 144 degrees of freedom ## Multiple R-squared: 0.9789, Adjusted R-squared: 0.9781 ## F-statistic: 1333 on 5 and 144 DF, p-value: &lt; 2.2e-16 The slope of the relationship between sepal length and petal length is calculated as follows, for each species: setosa \\((x_{2}=0, x_{3}=0): b_{1}=0.13\\) versicolor \\((x_{2}=1, x_{3}=0): b_{1} + b_{2} + b_{4} = 0.13+0.55 = 0.68\\) virginica \\((x_{2}=0, x_{3}=1): b_{1} + b_{3} + b_{5} = 0.13+0.62 = 0.75\\) Compare this to the estimates gained from the stratified model: coef(lm(Petal.Length ~ Sepal.Length, data=subset(iris, Species==&quot;setosa&quot;))) ## (Intercept) Sepal.Length ## 0.8030518 0.1316317 coef(lm(Petal.Length ~ Sepal.Length, data=subset(iris, Species==&quot;versicolor&quot;))) ## (Intercept) Sepal.Length ## 0.1851155 0.6864698 coef(lm(Petal.Length ~ Sepal.Length, data=subset(iris, Species==&quot;virginica&quot;))) ## (Intercept) Sepal.Length ## 0.6104680 0.7500808 They’re the same! Proof that an interaction is equivelant to stratification. So why do an interaction? Why not stratify? Stratification implies that the stratifying variable interacts with all other variables. Even variables that the variable is not directly interacting with. E.g. the stratified model below \\(Y = A + B + C + D + C*D\\), when D=1 \\(Y = A + B + C + D + C*D\\), when D=0 is the same as the following interaction model: \\(Y = A + B + C + D + A*D + B*D + C*D\\) "],
["adding-more-covariates-to-the-model.html", "3.6 Adding more covariates to the model", " 3.6 Adding more covariates to the model What if we now wanted to include other predictors in the model? How does sepal length relate to petal length after controlling for petal width? We add the variable for petal width into the model summary(lm(Petal.Length ~ Sepal.Length + setosa + Sepal.Length*setosa + Petal.Width, data=iris)) ## ## Call: ## lm(formula = Petal.Length ~ Sepal.Length + setosa + Sepal.Length * ## setosa + Petal.Width, data = iris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.83519 -0.18278 -0.01812 0.17004 1.06968 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.86850 0.27028 -3.213 0.00162 ** ## Sepal.Length 0.66181 0.05179 12.779 &lt; 2e-16 *** ## setosa 1.83713 0.62355 2.946 0.00375 ** ## Petal.Width 0.97269 0.07970 12.204 &lt; 2e-16 *** ## Sepal.Length:setosa -0.61106 0.12213 -5.003 1.61e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2769 on 145 degrees of freedom ## Multiple R-squared: 0.9761, Adjusted R-squared: 0.9754 ## F-statistic: 1478 on 4 and 145 DF, p-value: &lt; 2.2e-16 So far, petal width, and the combination of species and sepal length are both significantly associated with petal length. Note of caution: Stratification implies that the stratifying variable interacts with all other variables. So if we were to go back to the stratified model where we fit the model of petal length on sepal length AND petal width, stratified by species, we would be implying that species interacts with both sepal length and petal width. "],
["variable-selection-process.html", "3.7 Variable Selection Process", " 3.7 Variable Selection Process We want to choose a set of independent variables that both will yield a good prediction using as few variables as possible. In many situations where regression is used, the investigator has strong justification for including certain variables in the model. previous studies accepted theory The investigator may have prior justification for using certain variables but may be open to suggestions for the remaining variables. The set of independent variables can be broken down into logical subsets The usual demographics are entered first (age, gender, ethnicity) A set of variables that other studies have shown to affect the dependent variable A third set of variables that could be associated but the relationship has not yet been examined. Partially model-driven regression analysis and partially an exploratory analysis. 3.7.1 Selection Criteria 3.7.2 Coefficient of Determination If the model explains a large amount of variation in the outcome that’s good right? So we could consider using \\(R^{2}\\) as a selection criteria and trying to find the model that maximizes this value. The residual sum of squares (RSS in the book or SSE) can be written as \\(\\sum(Y-\\hat{Y})^{2}(1-R^{2})\\). Therefore minimizing the RSS is equivalent to maximizing the multiple correlation coefficient. Multiple \\(R^{2}\\) Problem: The multiple \\(R^{2}\\) always increases as predictors are added to the model. Adjusted \\(R^{2}\\) Ok, so let’s add an adjustment, or a penalty, to keep this measure in check. \\(R^{2}_{adj} = R^{2} - \\frac{p(1-R^{2})}{n-p-1}\\) 3.7.3 Akaike Information Criterion (AIC) A penalty is applied to the deviance that increases as the number of parameters \\(p\\) increase. AIC = \\(-2LL + 2p\\) Smaller is better "],
["what-to-watch-out-for-1.html", "3.8 What to watch out for", " 3.8 What to watch out for Use previous research as a guide Variables not included can bias the results Significance levels are only a guide Perform model diagnostics after selection to check model fit. Use common sense: A sub-optimal subset may make more sense than optimal one "],
["glm.html", "Chapter 4 Generalized Linear Models", " Chapter 4 Generalized Linear Models Or should this be Generalized Linear Models? Would allow us to talk about Logistic, log-linear and Poisson models. "]
]
