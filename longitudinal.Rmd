
# Longitudinal Data {#longitudinal}

What is **Longitudinal Data**?

- Repeated measures over time on subjects
- Different than other ways of repeated measures on clustered data (like family members, or kids in a classroom)
- Other similar data structures include time series data. Typically only have one subject (one stock market, sales from one product), and usually measured for a longer length of time. 
- **Key feature** is that longitudinal data is ordered over time, linear distance, or some other "meta-meter" such as trial number, dose or length. 

What can we learn from longitudinal data? 

- Population (average) response over time. 
- Population average response over time between different groups.
- How individual responses behave over time. 
- What is the estimate for the next observation over time? 
- How do covariates affect the population mean time trend and individual variation? 


The rest of this chapter is organized as follows: 

1. Introduce the **Pediatric Pain** experiment and explore the resulting data. 
2. Explore some basic analyses that are often conducted on longitudinal data
3. Conclude with fitting a random intercept model on this data. 



## Pediatric pain data
```{r, echo=FALSE}
library(haven)
pain <- read_sas("D:/School Files/Coursework/236 Repeated Measures/lab 3/pain.sas7bdat")
pain <- data.frame(pain)
```

The Pediatric Pain data set used in this chapter is a result of a designed experiment. Most studies on human pain are observational. The data consist of up to four observations on 64 children aged 8-10. The response is the length of time in seconds that the child can tolerate keeping his or her arm in very cold water, a proxy measure of pain tolerance. After the cold becomes intolerable, the child removes his or her arm. The arm is toweled off, no harm is done.

Two measurements were done on the first visit, then another 2 measurements were taken two weeks later during a second visit. After the arm was dried off study staff asked the child what they were thinking about during that trial. Children were classified into two groups depending on the child's coping style during the trial. _Attenders_ thought about the experiment, objects related to the experiment or their arm. _Distracters_ distracted themselves during the test by thinking about something unrelated to the experiment: the wall, homework, their pet, etc. 

A treatment was administered before the fourth trial. This treatment was a 10 minute conversation where the child was either counseled to attend to the arm, distract themselves from the arm, or no advice was given. 

> The study authors were interested in the main effects of treatment, coping style, and the interaction between the two. 

### Data sample 

```{r}
kable(pain[1:10,])
```

* `id`: participant ID
* `ses`: child's socio-economic status
* `cs`: coping style. Assessed and assigned at the first visit. 
    - `Attender` : thought about the experiment, objects related to their their arm, 
    - `Distracter` : distracted themselves during the test 
* `treatment`: Treatment group.  
    - `Attend` advised to pay attention to the arm
    - `Distract` advised to pay attention to something other than the arm
    - `no direction` no advice was given. 
* `trial`: 1, 2, 3, 4 (time)
* `paintol` = The length of time in seconds arm was underwater. Proxy measure of pain tolerance.
* `l2paintol` = $\log_{2}(paintol)$ - Normality transformation.

### Univariate Visualizations

* **Pain tolerance**

The raw score for `paintol` is very skewed right. A $log_{2}$ transformation was applied and found sufficient to achieve approximate normality. 

```{r}
pain.dist <- ggplot(pain, aes(x=paintol)) + geom_density() 
l2pain.dist <- ggplot(pain, aes(l2paintol)) + geom_density()
grid.arrange(pain.dist, l2pain.dist)
```

* **Coping style and treatment**

There are nearly equal proportions of coping styles and treatment types. 

```{r}
kable(table(pain$cs, pain$treatment))
```

## Visualizing longitudinal data

### Profile or spaghetti plots
How does individual's pain tolerance change over time/trials? What does the average trend look like? 

```{r, fig.width=10}
all <- ggplot(pain, aes(trial, l2paintol)) + geom_point(alpha=.5) + 
        geom_path(aes(group=id), alpha=.5) + geom_smooth(col="blue", lwd=2)
few <- ggplot(pain[1:16,], aes(trial, l2paintol, group=id)) + geom_line() + geom_point() 
grid.arrange(all, few, ncol=2)
```

The average is around 5, with a slight decrease at time 2 and then relatively constant. The shading around the trendline shows that the variance across time points is pretty constant.  The right hand plot shows the profile plot for the first 4 kids in the data set, notice how one path stops after trial 2? This tells us there is missing data in this data set that we will have to look out for. 

Quite a bit of variation across individuals, but what about the variation within person? I.e. how does the individual pain tolerance at each trial differ from the individual's average pain tolerance. We'll _center_ the data around each person's own average. With longitudinal data - the person is the cluster. 

```{r}
person.ave <- pain %>% group_by(id) %>% summarise(l2.ave = mean(l2paintol, na.rm=TRUE))
pain <- pain %>% left_join(person.ave) %>% mutate(diff = l2.ave-l2paintol)
# Always be very cautious when overwriting your data object! 
# Novice users recommended to create a new name.

ggplot(pain, aes(trial, diff)) + geom_path(aes(group=id), alpha=.5) + 
      geom_hline(yintercept=0, col="blue", lwd=2)
```

Few individuals have large variations in their pain tolerance across time, and it appears as if those that had higher than average at trial 2, were lower than average at trial 3, and vice versa. 

### Correlation of pain tolerance within person across time {#corrmat}
To calculate the correlation of `l2paintol` between trials, we need to reshape the data to wide format. 
In wide data, the measure of pain tolerance for each trial is it's own variable. 

```{r, fig.height=5, fig.width=5, fig.align='center'}
# First select only the variable we want to transform wide
pain.2.wide <- pain %>% select(id, trial, l2paintol)
# Use the reshape() function to transform - this comes with baseR
pw <- reshape(pain.2.wide, idvar="id", timevar="trial", direction="wide")
# then calculate and visualize the correlation of pain tolerance across time points. 
corrplot::corrplot.mixed(cor(pw[,-1], use = 'pairwise.complete.obs'))
```

There is a fairly high correlation of pain tolerance between trials. 

##  Analyze pain tolerance across coping style

Visualize the distribution of pain tolerance across the different coping styles. 

```{r, fig.width=10, fig.height=4}
pt.dist   <- ggplot(pain, aes(paintol, fill=cs)) + geom_density(alpha=.5)
l2pt.dist <- ggplot(pain, aes(l2paintol, fill=cs)) + geom_density(alpha=.5)
grid.arrange(pt.dist, l2pt.dist, ncol=2)
```

There appears to be a slight difference, but is it sigificantly different? 
What about the treatment? How does it appear to affect the pain tolerance of the coping styles? 

```{r}
ggplot(pain, aes(l2paintol, x=cs, fill=treatment)) + 
  geom_boxplot(width=1, alpha=.5) + geom_violin(alpha=.3) + 
  stat_summary(fun.y=mean, colour="black", geom="point", shape=18, size=4, position=position_dodge(width=.9))
```

Attenders seem to lower pain tolerance, and lower variation in that pain tolerance. The exception is attenders who were under the "no directions" treatment group. Distracters who were adviced to distract themselves were able to keep their arm under water for much longer than the other groups. 

We'll look at many different ways to approach this analysis. 

### Two Sample T-test
Compare pain tolerance across coping styles at each trial. 

```{r, fig.width=10}
ggplot(pain, aes(l2paintol, fill=cs)) + geom_density(alpha=.5) + facet_wrap(~trial)
```

There is a lot of overlap. Which differences are significant? We have no preconceived notion whether or not we can assume equal variances across groups, so we'll run t-tests under both conditions. 

```{r}
# Make an empty data frame to hold the p-values from the t-tests below.
ttst <- data.frame(eq.var=rep(NA,4), uneq.var=rep(NA,4), 
                   row.names=paste0("Trial", 1:4))
kable(ttst) # what does this look like empty? 

# loop over 1 to 4 trials, conduct a t.test, extract the p-value and store it into this empty data frame
for(t in 1:4){
  ttst$eq.var[t]   <- t.test(l2paintol~cs, data=subset(pain, trial==t), var.equal=TRUE)$p.value
  ttst$uneq.var[t] <- t.test(l2paintol~cs, data=subset(pain, trial==t), var.equal=FALSE)$p.value
}
kable(ttst, type='html', digits=4,  
      caption="T-test p-values for a difference between averge pain tolerance across coping style groups") %>% 
    kable_styling(bootstrap_options = "striped", full_width = F, position = "center") # now it's full
```

Conclusion: regardless if we assume equal or unequal variances, there is a difference in the log2 pain tolerance between attending groups during trial 1 and 2 only, not during 3 or 4. 

### Averaged over the observed baseline values. 
Perhaps the difference we're seeing is due to a difference at baseline between the two groups? Lets compute the baseline average over the first three responses and see if that changes across groups. 

```{r}
baseline <- pain %>% filter(trial < 4) %>% 
             group_by(cs, trial) %>% 
             summarise(ave.baseline.tol = mean(l2paintol, na.rm=TRUE))

ggplot(baseline, aes(ave.baseline.tol, fill=cs)) + geom_density(alpha=.5)
```

Pretty clear baseline difference isn't it? 
```{r}
pander(t.test(ave.baseline.tol ~ cs, data=baseline))
```

Children who distracted themselves from the experiment could keep their arm in the ice water for an average of 5.04 seconds. This was signficantly longer compared to children who attended to the experiment, they averaged 4.5 seconds keeping their arm in the ice water (p=.005). 


### Missing Data

We noticed earlier that there were some missing values in the data. Turns out that no one dropped out for reasons related to the experiment (i.e. they were uncomfortable), but there were some absences from school that day and a few broken arms. 

So far we've been ignoring that there is missing data. The table below shows that there are 6 individuals who are missing a score at time 3, but only 3 at time 4. This means that the time-stratified t-tests conducted earlier are on different sets of data. 

```{r}
kable(table(is.na(pain$l2paintol), pain$trial))
```

Is it the same individuals who are missing data? First let's identify the rows with any missing values and save that as a vector of row numbers into `miss.idx`.  

```{r}
miss.idx <- which(is.na(pain$l2paintol))
miss.idx
```

Then let's see the id's for those rows

```{r}
ids.with.missing <- pain$id[miss.idx]
ids.with.missing
```

There are 6 unique individuals who are missing measurements for pain tolerance. What does their data look like? 
Half male, half distracters, 3 in the no directions treatment group, 2 in the distraction treatment group, and 1 in the attend group. Understanding the difference in characteristics between those with missing data and those without is important - and a topic we'll come back to in a later chapter. 

```{r}
kable(pain[pain$id %in% ids.with.missing,c('id', 'cs', 'treatment', 'sex', 'trial', 'paintol' )], 
      type='html', row.names=FALSE) %>% 
      kable_styling(bootstrap_options = "striped", full_width = F, position = "center")
```

What if we just drop all records for all children with missing values at any time point? How does the results here compare to the previous test? First we have to go back and drop these 6 individuals from the data set _before_ we calculate the baseline mean. 

```{r}
base.nomiss <- pain %>% na.omit() %>% filter(trial < 4) %>% 
                group_by(cs, trial) %>% 
                summarise(ave.baseline.tol = mean(l2paintol))

ggplot(base.nomiss, aes(ave.baseline.tol, fill=cs)) + geom_density(alpha=.5)
pander(t.test(ave.baseline.tol ~ cs, data=base.nomiss))
```

The average shift a little bit, the bimodality of the distribution of baseline pain tolerance is more clear. The conclusion remains the same however, the p-value was nearly doubled but when it's already in the third digit, a move from 0.005 to 0.002 does not change your conclusion. 


## Difference of Differences (DoD)

So far we've assumed that pain tolerance within person is independent across time points. That could be an unreasonable assumption, so let's look analyzing the differences in individual responses between two time points. Then we are interested in understanding how those differences over time, are different between groups. hence **DoD**. 

For some of these analyses we'll need the data back into wide format. This time we'll apply `reshape` to the whole data set this time, turning _all_ variables wide. We'll keep using only the fully complete cases from here on out so that each analyses is on the same set of individuals. 

```{r}
pain.wide <- reshape(pain, idvar="id", timevar="trial", direction="wide") %>% na.omit()
names(pain.wide) # What does this data set look like now? 
```

Each variable now has 4 entries, one for each time point. Some variables such as `cs` are **time fixed**, meaning they don't change over time; `cs.1` = `cs.2` = `cs.3` = `cs=4`. Other variables like `paintol` are **time varying**, they change within person across time. 

### Paired t-test



## Might need textbook for this. 



## What about just the 4th observation minus the 2nd? 

```{r}
#t.test(data=base.nomiss)
```



## Random Intercept Model

The random intercept model combines aspects of the various simple analyses in one large model that uses all the data available and in an appropriate fashion. 


Since we have a specified correlation structure, we'll use the `lme` function found in the `nlme` package and change the correlation structure to an **Autoregressive with lag 1** (AR1) structure. The `value=` argument is an estimate of the value of the lag 1 autocorrelation. 

![](images/q.png) Where did the 0.7 come from? 


```{r}
library(nlme)
ar.model <- lme(l2paintol ~ cs + treatment + cs*treatment, 
            random = ~ 1|id, cor = corAR1(value = 0.7, form=~1|id),  
            data=pain, na.action=na.omit)
summary(ar.model)
```

* The model converged - don't forget to check this. There was no error message in the output saying it didn't converge, so we're good. 
* Recall this is an interaction model, so we cannot interpret the main effects directly. We have to write `contrasts` to test for any DoD. 


## Writing Contrasts

Conducting a DoD analysis from a regression model simplifies down to testing that some linear combination of beta's is different from zero. 
Before we can test a specific combinations of coefficients write down contrasts, we need to be able to write down a vector `$\mathbf{L}$` of **linear coefficients** for each group we want to compare such that when you multiply $\mathbf{L}\beta$ you are only left with the beta coefficients of interest. 

### Writing Linear Coefficient vectors

Since all covariates in this model are categorical, what is modeled is a series of binary indicator variables (and in this case the interaction between the two). The table below shows the matrix of $\beta$ regression coefficients. 
$$
\beta = 
\begin{bmatrix} 
4.66 \\ 0.20 \\ -0.13 \\ -0.23 \\ 0.81 \\ -0.09 
\end{bmatrix} 
$$

If our linear combination $L$ looked like this: 
$$
L = 
\begin{bmatrix} 
1 & 0 & 0 & 0 & 0 & 0 \\ 
\end{bmatrix} 
$$
then 

$$
L\beta = 
\begin{bmatrix} 
1 & 0 & 0 & 0 & 0 & 0 \\ 
\end{bmatrix} 
\begin{bmatrix} 
4.66 \\ 0.20 \\ -0.13 \\ -0.23 \\ 0.81 \\ -0.09 
\end{bmatrix}
=
1*4.66 + 0*0.20 - 0 * 0.13 - 0*0.23 + 0*0.81 - 0*0.09 = 4.66
$$

This is the estimate for the log base 2 seconds for an attender in the attending (AA) treatment group. 

The $L$ matrix for an attender in the distracting treatment group (AD) would look like: 
$$
L\beta = 
\begin{bmatrix} 
1 & 0 & 1 & 0 & 0 & 0 \\ 
\end{bmatrix} 
\begin{bmatrix} 
4.66 \\ 0.20 \\ -0.13 \\ -0.23 \\ 0.81 \\ -0.09 
\end{bmatrix}
=
1*4.66 + 0*0.20 - 1*0.13 - 0*0.23 + 0*0.81 - 0*0.09 = 4.53
$$

We can have R do this matrix multiplication `%*%` for us: 
```{r}
L = c(1,0,1,0,0,0)
B = fixed.effects(ar.model)
L%*%B
```

![](images/q.png) Write down the $L$ matrix and then calculate $L\beta$ for each of the following contrasts: 

* A distractor (D) in the distraction (D) treatment group. 
* A distractor (D) in the no direction (N) treatment group. 
* An attender (A) in the no direction (N) treatment group. 


### DoD: Subtracting $L_{1}$ from $L_{2}$

What we're really interested in, is the difference of differences. The difference AA-AD gives us the effect of the distracting treatment on attenders, relative to the attending treatment. 

$$
\ \ \ \ \ L_{1} = 
\begin{bmatrix} 
1 & 0 & 0 & 0 & 0 & 0 \\ 
\end{bmatrix} \\
\ \ \ \ \  L_{2} = 
\begin{bmatrix} 
1 & 0 & 1 & 0 & 0 & 0 \\ 
\end{bmatrix} \\
L_{1} - L_{2} = 
\begin{bmatrix} 
0 & 0 & -1 & 0 & 0 & 0 \\ 
\end{bmatrix} 
$$
```{r}
L = c(0,0,-1,0,0,0)
L%*%B
```

Attenders in the distraction treatment group can keep their arm in ice water on average $2^{0.12} = 1.09$ seconds longer compared to attenders in the attending treatment group. 

![](images/q.png) Using the $L$ matrices generated earlier, write down the linear combination of coefficients for the following DoD comparisons, calculate the estimate and interpret the results. 

* A distractor (D) in the distraction (D) treatment group. 
* A distractor (D) in the no direction (N) treatment group. 
* An attender (A) in the no direction (N) treatment group. 

* DD - DN
* DN - AN
* DD - AA



 ###### ----- write down enough L's for pair work in clss -----#comment most out. 

```{r}
library(multcomp)
contrast.formulas <- rbind(`Distractor` = c(1, 0,  0, 0, 0,0), 
                           `Tx: N-A` = c(0, 0,  1, 0, 0,0), 
                           `Tx: D-N` = c(0, 1, -1, 0, 0,0))


plot(glht(ar.model, linfct=contrast.formulas))


```



```{block2, type="rmdnote"}
**Side Note**  You can use the `intervals()` function from the `nlme` package to auto-generate correct confidence intervals for both the fixed effects and the variance terms. 

intervals(ar.model)
```

## Additional Resources

* Modeling Longitudinal Data, Robert E. Weiss (2005) https://www.springer.com/us/book/9780387402710 
* Modeling time series data https://www.datascience.com/blog/introduction-to-forecasting-with-arima-in-r-learn-data-science-tutorials
