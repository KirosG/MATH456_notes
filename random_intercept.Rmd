# Random Intercept Models {#RI}

<span style="color:blue">**Example Data**</span>

Radon is a radioactive gas that naturally occurs in soils around the U.S. As radon decays it releases other radioactive elements, which stick to, among other things, dust particles commonly found in homes.  The EPA believes [radon exposure](https://www.epa.gov/radon) is one of the leading causes of cancer in the United States.

This example uses a dataset named `radon` from the `rstanarm` package. The dataset contains $N=919$ observations, each measurement taken within a home that is located within one of the $J=85$ sampled counties in Minnesota.  The first six rows of the dataframe show us that the county Aitkin has variable levels of $log(radon)$. Each of the three models will predict $log(radon)$.

```{r}
data(radon, package="rstanarm")
head(radon)

```

## Pooling

To highlight the benefits of random intercepts models we will compare three linear regression models: 

* complete pooling
* no pooling
* partial pooling (the random intercept model)


**Complete Pooling**

The complete pooling model pools all counties together to give one single estimate of the $log(radon)$ level. 


**No Pooling**

No pooling refers to the fact that no information is shared among the counties.  Each county is independent of the next.


**Partial Pooling**

The partial pooling model, partially shares information among the counties. 

Each county should get a _unique intercept_ such that the collection of county intercepts are randomly sampled from a normal distribution with mean $0$ and variance $\sigma^2_{\alpha}$.

Because all county intercepts are randomly sampled from the same theoretical population, $N(0, \sigma^2_{\alpha})$, information is shared among the counties.  This sharing of information is generally referred to as **shrinkage**, and should be thought of as a means to reduce variation in estimates among the counties.  When a county has little information to offer, it's estimated intercept will be shrunk towards to overall mean of all counties.

```{r, echo=FALSE}

fit_nopool <- lm(log_radon~-1+county, data=radon)
fitted_nopool <- dplyr::bind_cols(radon,
                                       data.frame(.fitted=predict(fit_nopool)))

fit_partpool <- lme4::lmer(log_radon ~ (1 |county), data=radon)
fitted_partpool <- dplyr::bind_cols(radon,
                                    data.frame(.fitted=predict(fit_partpool),
                                               .fixed=lme4::fixef(fit_partpool)))

```

The plot below displays the overall mean as the complete pooling estimate (solid, horizontal line), the no pooling and partial pooling estimates for 8 randomly selected counties contained in the radon data.  The amount of shrinkage from the partial pooling fit is determined by a data dependent compromise between the county level sample size, the variation among the counties, and the variation within the counties.  


```{r, echo=FALSE, fig.width=8, fig.height=5, fig.align="center"}

county_idx <- sample(unique(radon$county), 8, replace=FALSE)
fitted_nopool %>%
    filter(county %in% county_idx) %>%
    ggplot(aes(x=county, y=.fitted, colour="not pooled")) +
    geom_jitter() +
    geom_point(data=filter(fitted_partpool, county %in% county_idx),
               aes(y=.fitted, colour="partially pooled")) +
    geom_hline(data=fitted_partpool, aes(yintercept=mean(.fixed), colour="completely pooled")) +
    labs(y="Estimated county means", x="County") +
    theme(axis.text.x=element_text(angle=35, hjust=1)) +
    guides(colour=guide_legend(title="Model"))


```

Generally, we can see that counties with smaller sample sizes are shrunk more towards the overall mean, while counties with larger sample sizes are shrunk less.  

```{block2, type="rmdcaution"}
The fitted values corresponding to different observations within each county of the no-pooling model are jittered to help the eye determine approximate sample size within each county. 

Estimates of variation within each county should not be determined from this arbitrary jittering of points.
```

## Mathematical Models

The three models considered set $y_n=log(radon)$, and $x_n$ records floor (0=basement, 1=first floor) for homes $n=1, \ldots, N$.  

### Complete Pooling

The complete pooling model pools all counties together to give them one single estimate of the $log(radon)$ level, $\hat{\alpha}$.  

* The error term $\epsilon_n$ may represent variation due to measurement error, within-house variation, and/or within-county variation.  
* Fans of the random intercept model think that $\epsilon_n$, here, captures too many sources of error into one term, and think that this is a fault of the completely pooled model.


\begin{equation*}
\begin{split}

        y_n = \alpha & + \epsilon_n \\
            & \epsilon_n \sim N(0, \sigma_{\epsilon}^{2})

\end{split}
\end{equation*}


### No Pooling

* The no pooling model gives each county an independent estimate of  $log(radon$), $\hat{\alpha}_{j[n]}$.  
* Read the subscript $j[n]$ as home $n$ is nested within county $j$.  Hence, all homes in each county get their own independent estimate of $log(radon)$.  
* This is equivalent to the fixed effects model
* Here again, one might argue that the error term captures too much noise.


\begin{equation*}
\begin{split}

        y_n = \alpha_{j[n]} & + \epsilon_n \\
            \epsilon_n & \sim N(0, \sigma_{\epsilon}^{2})

\end{split}
\end{equation*}

### Partial Pooling (RI)

* The random intercept model, better known as the partial pooling model, gives each county an intercept term $\alpha_j[n]$ that varies according to its own error term, $\sigma_{\alpha}^2$.  
* This error term measures within-county variation
    - Separating measurement error ($\sigma_{\epsilon}^{2}$) from county level error ($\sigma_{\alpha}^{2}$) . 
* This multi-level modeling shares information among the counties to the effect that the estimates $\alpha_{j[n]}$ are a compromise between the completely pooled and not pooled estimates.  
* When a county has a relatively smaller sample size and/or the variance $\sigma^{2}_{\epsilon}$ is larger than the variance $\sigma^2_{\alpha}$, estimates are shrunk more from the not pooled estimates towards to completely pooled estimate.


\begin{equation*}
\begin{split}

        y_n = \alpha_{j[n]} & + \epsilon_n \\
            \epsilon_n & \sim N(0, \sigma_{\epsilon}^{2}) \\
            \alpha_j[n] & \sim N(\mu_{\alpha}, \sigma_{\alpha}^2)

\end{split}
\end{equation*}


## Fitting models in R

**No Pooling**

The no pooling model is fit with the function `lm`, giving each county a unique intercept in the model.  
```{r}
fit_nopool <- lm(log_radon ~ -1 + county, data=radon)
```

**Partial Pooling**

* The partial pooling model is fit with the function `lmer()`, which is part of the `lme4` package.
* The extra notation around the input variable `(1|county)` dictates that each county should get its own unique intercept $\alpha_{j[n]}$. 

```{r}
fit_partpool <- lme4::lmer(log_radon ~ (1 |county), data=radon)
```

The fixed effects portion of the model output of `lmer` is similar to output from `lm`, except no p-values are displayed.  The fact that no p-values are displayed is a much discussed topic.  The author of the library `lme4`, Douglas Bates, believes that there is no "obviously correct" solution to calculating p-values for models with randomly varying intercepts (or slopes); see [here](https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html) for a general discussion. 

```{r}
summary(fit_partpool)
```

The random effects portion of the `lmer` output provides a point estimate of the variance of component $\sigma^2_{\alpha} = 0.0$ and the model's residual variance, $\sigma_\epsilon = 0.57$.


### Restricted (residual) Maximum Likelihood (REML)

* Similar to logistic regression, estimates can't be estimated directly using maximum likelihood (ML) methods. 
* Iterative methods like REML are needed to get approximations. 
* REML is typically the default estimation method for most packages. 

Details of REML are beyond the scope of this class, but knowing the estimation method is important for two reasons

1. Some type of testing procedures that use the likelihood ratio may not be valid. 
    - Comparing models with different fixed effects using a likelihood ratio test is not valid. (Must use Wald Test instead) 
    - Can still use AIC/BIC as guidance (not as formal tests)

2. Iterative procedures are procedures that perform estimation steps over and over until the change in estimates from one step to the next is smaller than some tolerance.
    - Sometimes this convergence to an answer never happens. 
    - You will get some error message about the algorithm not converging. 
    - The more complex the model, the higher chance this can happen
    - scaling, centering, and avoiding collinearity can alleviate these problems with convergence.







## Including Covariates

A similar sort of shrinkage effect is seen with covariates included in the model.  

Consider the covariate $floor$, which takes on the value $1$ when the radon measurement was read within the first floor of the house and $0$ when the measurement was taken in the basement. In this case, county means are shrunk towards the mean of the response, $log(radon)$, within each level of the covariate.

```{r, echo=FALSE, fig.width=8, fig.height=5, fig.align="center"}

radon$floor <- factor(radon$floor, labels=c("basement", "first floor"))
fit_nopool <- lm(log_radon ~ floor + county, data=radon)
fitted_nopool <- dplyr::bind_cols(radon,
                                  data.frame(.fitted=predict(fit_nopool)))


fit_partpool <- lme4::lmer(log_radon ~ floor + (1 |county), data=radon)
fitted_partpool <- dplyr::bind_cols(radon,
                                    data.frame(.fitted=predict(fit_partpool),
                                               .fixed=predict(fit_partpool, re.form=NA)))

county_idx <- c("BLUEEARTH", "DAKOTA", "FARIBAULT", "HENNEPIN",
                "LACQUIPARLE", "MARSHALL", "PINE", "WASECA")

fitted_nopool %>%
    filter(county %in% county_idx) %>%
    ggplot(aes(x=county, y=.fitted, colour="not pooled")) +
    geom_jitter() +
    geom_point(data=filter(fitted_partpool, county %in% county_idx), aes(y=.fitted, colour="partially pooled")) +
    geom_hline(data=filter(fitted_partpool, county %in% county_idx), aes(yintercept=.fixed, colour="completely pooled")) +
    facet_grid(.~floor) +
    labs(y="Estimated county means", x="County") +
    theme(axis.text.x=element_text(angle=35, hjust=1)) +
    guides(colour=guide_legend(title="Model"))

```

Covariates are fit using standard `+` notation outside the random effects specification, i.e. `(1|county)`. 

```{r}
library(sjPlot)
ri.with.x <- lme4::lmer(log_radon ~ floor + (1 |county), data=radon)
sjt.lmer(ri.with.x)
```

The estimated random effects can also be easily visualized. 

```{r}
plot_model(ri.with.x, type="re", sort.est = "(Intercept)", y.offset = .4)
```

Function enhancements -- (see [vignette](http://www.strengejacke.de/sjPlot/) for more options) 

* Display the fixed effects by changing `type="est"`. 
* Plot the slope of the fixed effect for each level of the random effect `sjp.lmer(ri.with.x, type="ri.slope")` -- this is being depreciated in the future but works for now. Eventually I'll figure out how to get this plot out of `plot_model()`. 


## Components of Variance

Statistics can be thought of as the study of uncertainty, and variance is a measure of uncertainty (and information). So yet again we see that we're partitioning the variance. Recall that 

* Measurement error: $\sigma^{2}_{\epsilon}$ -- This was just $\sigma^{2}$ in the previous model output
* County level error: $\sigma^{2}_{\alpha}$ -- This is listed as $\tau_{00}$ in the previous model output

The **intraclass correlation** (ICC, $\rho$) is interpreted as

* the proportion of total variance that is explained by the clusters.  
* the expected correlation between two individuals who are drawn from the same cluster. 

$$ 
\rho = \frac{\sigma^{2}_{\alpha}}{\sigma^{2}_{\alpha} + \sigma^{2}_{\epsilon}}
$$

## Centering terms

* Sometimes it might be better to measure the effect of a specific level relative to the average within cluster, rather than overall average.
* The "frog pond" effect
    - A student with an average IQ may be more confident and excel in a group of students with less than average IQ
    - But they may be discouraged and not perform to their potential in a group of students with higher than average IQ.
    
* Instead of using the actual value in the regression model
    - calculate the cluster specific average
    - calculate the difference between individual and specific cluster average
    - both cluster average (macro) and difference (micro) are included in the model. 




## Additional References

* http://stla.github.io/stlapblog/posts/AV1R_SASandR.html
* https://www.theanalysisfactor.com/the-intraclass-correlation-coefficient-in-mixed-models/


* [sjPlot](http://strengejacke.de/sjPlot/sjt.lmer/) **Really** nice way of printing output as tables (and plots).
* [visreg](http://pbreheny.github.io/visreg/mixed.html ) - a R package for visualization of regression models (not so super useful here, but worth a scan) 
* Interesting blog by [Tristan Mahr](https://tjmahr.github.io/plotting-partial-pooling-in-mixed-effects-models/) about pooling and shrinkage. 